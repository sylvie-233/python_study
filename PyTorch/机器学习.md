# 机器学习

>Author: Sylvie233
>
>Date: 2022/11/27
>
>Point: 

[TOC]

## 基础介绍

**机器学习算法分类：**

输入数据有无标签：

1. 监督学习
   - 分类（离散型）：k-近邻算法、贝叶斯分类、决策树与随机森林、逻辑回归、<u>神经网络</u>
   - 回归（连续型）：线性回归、岭回归
   - 标注：隐马尔可夫模型
2. 无监督学习
   - 聚类：k-means、







机器学习领域：

1. 传统预测
2. 自然语言处理
3. 图像识别



分类问题、值预测、



用户数据、数据清洗、数据预处理、特征工程、机器学习、模型评估、离线/在线服务

numpy、pandas、sklearn、



数据集：

文件csv

scikit-learn、UCI、Kaggle可用数据集

特征值、目标值

sklearn数据集

.pkz文件





**特征工程：**特征提取、数据预处理（归一化、标准化）、数据降维

特征值化

sparse矩阵、scipy工具

one-hot编码、



文本特征提取：

文本分类、情感分析

中文处理：jieba分词

```
pip3 install jieba

import jieba

_str = jieba.cut("中文字符串")
_content= list(_str)
_str_new = ' '.join(_content)
```

文本分类：

tf：term frequency词频

idf：inverse document frequency逆文档频率（log(总文档数/该词出现的文档数)）

count、tf-idf算法

重要性=tf * idf



数据预处理：

特征预处理

通过特定的统计方法进行转换

1. 归一化、标准化、缺失值
2. one-hot编码
3. 时间的切分

归一化：映射到[0,1]之间

使得特征值的大小不会直接干扰目标值的判断

异常值处理

标准化：通过对原始数据进行变换，把数据变换到均值为0，方差为1的范围类（平均值、标准差）

使用标准化处理异常点

缺失值处理：删除、插补（平均数、中位数）

np.nan



数据降维：特征的数量

特征选择：

- 过滤式Filter：方差域、
- 嵌入式Embedded：正则化、决策树
- 包裹式Wrapper

神经网络特征选择

主成分分析

线性判别分析





### 转换器

fit_transform()转换数据集





### 估计器

模型训练、预测、预测的准确性





### 精确率

分类模型评估（准确率）

<img src="机器学习.assets/image-20221128160354156.png" alt="image-20221128160354156" style="zoom:67%;" />

**对每个类别都有一个混淆矩阵**

**召回率**

 <img src="机器学习.assets/image-20221128161036195.png" alt="image-20221128161036195" style="zoom:67%;" />

**F1-Score**

<img src="机器学习.assets/image-20221128161410786.png" alt="image-20221128161410786" style="zoom:67%;" />







### 交叉验证

训练集、验证集

数据n等分，n等分中训练集和验证集逐渐交换位置，共进行n次训练，结果值取n次交叉验证的平均值

<img src="机器学习.assets/image-20221128162443020.png" alt="image-20221128162443020" style="zoom:50%;" />

**网格搜索**

调参数，使用交叉验证对超参数组进行评估，择优

<img src="机器学习.assets/image-20221128162723024.png" alt="image-20221128162723024" style="zoom:50%;" />





### 损失函数

<img src="机器学习.assets/image-20221129194927235.png" alt="image-20221129194927235" style="zoom:67%;" />





### 过拟合/欠拟合

<img src="机器学习.assets/image-20221129203634621.png" alt="image-20221129203634621" style="zoom:67%;" />

<img src="机器学习.assets/image-20221129203659536.png" alt="image-20221129203659536" style="zoom:67%;" />

正则化：

减少权重、趋近于0、

<img src="机器学习.assets/image-20221129204007272.png" alt="image-20221129204007272" style="zoom:67%;" />





### 模型保存/加载





### 梯度下降

<img src="机器学习.assets/image-20221130101035060.png" alt="image-20221130101035060" style="zoom: 67%;" />













## 核心算法

### 主成分分析

特征数据降维，

PCA

特征选择、数据降维、简化数据集 



### K-近邻算法

 根据邻居判断类别

特征空间k个最相似中所属类别最多的

欧氏距离

<img src="机器学习.assets/image-20221128114809088.png" alt="image-20221128114809088" style="zoom:80%;" />

**k值的决定**（超参数，外面指定）



### 朴素贝叶斯

划分类别概率最大值（**特征之间（关键词）条件独立**）

联合概率、条件概率

<img src="机器学习.assets/image-20221128153240631.png" alt="image-20221128153240631" style="zoom: 67%;" />

<img src="机器学习.assets/image-20221128153527327.png" alt="image-20221128153527327" style="zoom: 67%;" />

训练统计训练集中的各类别的词频统计， 对被预测文档进行词频统计，计算各个类别的概率，最后选择概率最大的

<img src="机器学习.assets/image-20221128154415288.png" alt="image-20221128154415288" style="zoom: 67%;" />

拉普拉斯平滑系数，处理概率为0的情况

<img src="机器学习.assets/image-20221128154606010.png" alt="image-20221128154606010" style="zoom:67%;" />

tf-idf词频统计：利用训练集建立词频列表 

训练集很大影响预测结果





### 决策树/随机森林

根据**信息增益大小对特征字段**的判断顺序进行**排序**

信息论、bit比特、信息熵、不确定性

决策树划分：

信息增益：当得知一个特征值的信息时，信息熵的减少的大小

<img src="机器学习.assets/image-20221128165606053.png" alt="image-20221128165606053" style="zoom:50%;" />

<img src="机器学习.assets/image-20221128165635327.png" alt="image-20221128165635327" style="zoom:50%;" />

信息熵：对目标值进行计算

条件熵：对该特征值的各个取值范围求信息熵，最后求和（Di为该特征值该取值范围拥有的个数）

信息增益：对各个特征值进行计算



决策树算法：

<img src="机器学习.assets/image-20221128170509843.png" alt="image-20221128170509843" style="zoom:50%;" />

**减枝cart算法**

过拟合问题



**随机森林**

集成学习方法：<u>生成多个分类器/模型</u>，各自独立地学习和作出预测、这些预测最后结合成单预测

随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定

建立多个决策树：

1. 随机在N个样本当中选择一个样本，重复N次（样本有可能重复）
2. 随机在M个特征当中选出m的特征
3. 完成一棵决策树的建立（重复1-3步骤）

有放回抽样

能够有效地运行在大数据集上







### 线性回归/岭回归

求权重、偏置（n维 ）、正规方程、梯度下降

目标值为连续型数值

线性关系

<img src="机器学习.assets/image-20221129193106721.png" alt="image-20221129193106721" style="zoom: 67%;" />

线性回归：

<img src="机器学习.assets/image-20221129193256299.png" alt="image-20221129193256299" style="zoom: 67%;" />



迭代式算法、损失函数

算法、策略、优化

**最小二乘法**：误差平方和

寻找使得损失最小的w权重组合（**正规方程**）

<img src="机器学习.assets/image-20221129195209431.png" alt="image-20221129195209431" style="zoom: 67%;" />

**梯度下降：**α学习速率、cost损失函数

利用损失函数逐步减少的方向更新w权重参数

<img src="机器学习.assets/image-20221129195547191.png" alt="image-20221129195547191" style="zoom:67%;" />

w的初始值可随机选取

梯度下降的过程（减小损失函数值）即学习的过程



标准化处理特征数据、目标值数据

预测好的目标值再反向转换为原始目标值 （inverse_transform()）



均方误差：

<img src="机器学习.assets/image-20221129202054678.png" alt="image-20221129202054678" style="zoom: 67%;" />



**岭回归Ridge：**

带有正则化的线性回归















### 逻辑回归

分类算法、**二分类**

<img src="机器学习.assets/image-20221129205735223.png" alt="image-20221129205735223" style="zoom:67%;" />

sigmoid函数：

<img src="机器学习.assets/image-20221129205902923.png" alt="image-20221129205902923" style="zoom:67%;" />

分类概率值、阈值（概率大于该阈值则为该类、否则为另一类）：

<img src="机器学习.assets/image-20221129210037403.png" alt="image-20221129210037403" style="zoom:67%;" />

损失函数：对数似然损失

<img src="机器学习.assets/image-20221129210241093.png" alt="image-20221129210241093" style="zoom:67%;" />

m个样本分类的损失值求和



使用梯度下降优化

对数似然损失存在多个局部最小值

解决方法（多次随机初始化、调整学习率）



生成模型（先验概率）、判别模型

















### K-means聚类

分类、非监督学习（无标签目标值）

K：把数据划分成多少个类别

<img src="机器学习.assets/image-20221129215154700.png" alt="image-20221129215154700" style="zoom:67%;" />

评估标准：轮廓系数

外部距离最大化，内部距离最小化：[-1,1]

<img src="机器学习.assets/image-20221129220347530.png" alt="image-20221129220347530" style="zoom:67%;" />

越趋近于1，聚类效果越好



容易有局部最优解







































### 神经网络

神经网络种类：

- 基础神经网络：单层感知机、线性神经网络、BP神经网络、Hopfield神经网络
- 进阶神经网络：玻尔兹曼机、受限玻尔兹曼机、递归神经网络
- 深度神经网络：深度置信网络、<u>卷积神经网络</u>、循环神经网络、LSTM网络



人工神经网络（artificial neural network）ANN

<img src="机器学习.assets/image-20221130212522149.png" alt="image-20221130212522149" style="zoom:67%;" />

组成结构：

<img src="机器学习.assets/image-20221130212727566.png" alt="image-20221130212727566" style="zoom:67%;" />

**多分类**、**softmax**、



权重、偏置

权重的个数即神经元连线的条数（n维特征*m个类别）（\[n, m]），偏置的个数即分类的个数



<img src="机器学习.assets/image-20221201095626949.png" alt="image-20221201095626949" style="zoom:67%;" />

















#### 感知机

<img src="机器学习.assets/image-20221130210026983.png" alt="image-20221130210026983" style="zoom:67%;" />

**分类问题**、激活函数sigmoid、与或问题



#### SVM



#### SoftMax回归

<img src="机器学习.assets/image-20221130214240190.png" alt="image-20221130214240190" style="zoom:67%;" />

（i、j为权重、偏置计算后的值）

one-hot编码、类别判断

<img src="机器学习.assets/image-20221130214621186.png" alt="image-20221130214621186" style="zoom:67%;" />







#### 交叉熵损失

<img src="机器学习.assets/image-20221130214832155.png" alt="image-20221130214832155" style="zoom:67%;" />

一个样本（类别）就有一个交叉熵损失，所有类别求和，对n行数据的交叉熵损失求平均值







#### 反向传播算法

梯度下降

正向传播：经过一层层计算得到输出值





#### 准确率





#### 卷积神经网络

<img src="机器学习.assets/image-20221201100359644.png" alt="image-20221201100359644" style="zoom:67%;" />

发展历史：

<img src="机器学习.assets/image-20221201095955219.png" alt="image-20221201095955219" style="zoom:67%;" />

结构：

<img src="机器学习.assets/image-20221201100550253.png" alt="image-20221201100550253" style="zoom:67%;" />

<img src="机器学习.assets/image-20221201100238379.png" alt="image-20221201100238379" style="zoom: 67%;" />

零填充：

![image-20221201101618046](机器学习.assets/image-20221201101618046.png)

 激活函数：Relu

偏置（个数为卷积核的个数，卷积计算即权重计算）计算和激活函数计算同时进行

卷积计算后要立即加偏置（偏置计算的权重为(1x1x1... （个数为通道数）)）

<img src="机器学习.assets/image-20221201103333277.png" alt="image-20221201103333277" style="zoom:67%;" />

池化层：

<img src="机器学习.assets/image-20221201104028433.png" alt="image-20221201104028433" style="zoom:67%;" />

全连接层：

全连接进行的是直接矩阵运算加偏置

<img src="机器学习.assets/image-20221201104521245.png" alt="image-20221201104521245" style="zoom:67%;" />



在模型中封装各层之间的运算（输入层 -> 输出层）



全连接层的输出再经过softmax处理成\[0,1]的概率值

计算后的概率值再 进行交叉熵损失计算

利用loss损失值进行梯度下降反向传播优化

最后在进行准确率的统计



dropout删除一些数据防止过拟合





常用的卷积神经网络结构：

- LeNet
- AlexNet
- GoogleNet









## Scikit-learn库

安装：

```
pip install Scikit-learn


```



算法：

1. 分类
2. 回归
3. 聚类
4. 降维
5. 模型选择
6. 特征工程



常用API

```
聚类：(sklearn.cluster)
sklearn.cluster
	KMeans: k-means聚类
		():
			n_clusters: 开始的聚类中心数量
			init: 初始化方法
		return:
		
		fit():
			x_train:
		predict():
			x_train:
			
		

集成学习算法：(sklearn.ensemble)
sklearn.ensemble
	RandomForestClassifier: 随机森林
		():
			n_estimators: 树的个数
			criteria: 分割特征的测量方法
			max_depth: 
			max_features: 每个决策树的最大特征数量
			bootstrap: 是否有放回抽样

扩展：(sklearn.externals)
sklearn.externals
	joblib: 模型保存、加载
		dump():
			estimator: 保存的模型
			name: "xxx.pkl"（保存的文件）
		load():
			file: "xxx.pkl"（加载的模型文件）
			return: estimator
		

数据集：(sklearn.datasets)
sklearn.datasets
	load_*(): 小规模（iris、）
		load_iris():
		load_boston():
			
	fetch_*(): 大规模
		fetch_20newsgroups()
			subset: "all"/
	
    datasets.base.Bunch: 字典数据集
        data: 特征值
        target: 目标值（标签）
        DESCR:
        feature_names: 特征名
        target_names: 标签名

降维：(sklearn.decompostion)
sklearn.decomposition
	PCA: 主成分分析
		(n_components=小数/整数（降低比例）)
		fit_transform()

特征工程：(sklearn.feature_extraction)
sklearn.feature_extraction
	DictVectorizer: 字典特征提取
		(sparse=False, )
		get_feature_names()
		fit_transform(): 各行特征值的取值转换为one-hot编码，并生成对应列
		fit()

sklearn.feature_extraction.text
	CountVectorizer: 文本特征提取（词频统计）
		fit_transform()
		get_feature_names(): 
		inverse_transform(): 反向转换
		
	TfidfVectorizer: 词频重要性统计
		fit_transform():
		get_featur_names():

特征选择：(sklearn.feature_selection)
sklearn.feature_selection 
	VarianceThreshold: 删除低方差特征
		(threshold=1.0)
		fit_transform()

线性模型：(sklearn.linear_model)
sklearn.linear_model
	LinearRegression: 正规方程
		():
		return:
			coef_: 权重系数（回归系数）
		
		fit():
			x_train:
			y_train:
		predict():
			x_test:
			return:
				y_test
				
	LogisticRegression: 逻辑回归
		():
			C: 正则化
		return:
			coef_: 回归系数
		
		score():
			x_test:
			y_test:
			
				
	Ridge: 岭回归
		(): 
			alpha: 正则化力度
		return:
			coef_: 回归系数
		
		fit():
			x_train:
			y_train:
		predict():
			x_test:
			return:
				y_pred
	
	SGDRegressor: 梯度下降
		():
		return: 
			coef_: 回归系数
		
		fit():
			x_train:
			y_train:
		predict():
			x_train:
			return:
				y_train:

评估指标：(sklearn.metrics)
sklearn.metrics
	classification_report(): 分类的精确率、召回率
		y_true: 真实目标值
		y_pred: 预测目标值
		labels: 目标标签值
		target_names: 目标类别名称
		return: 每个类别的精确率和召回率

	mean_squared_error(): 均方误差
		y_true:
		y_pred:
		return:
		
	silhouette_score(): 轮廓系数
		X: 特征值
		
			

模型选择：(sklearn.model_selection)
sklearn.model_selection
	train_test_split(*arr（特征值）, *arr_2（目标值）, **opts): 数据集划分
		x: 特征值
		y: 目标值
		opts:
            test_size: 测试集大小
		return: (x_train, x_test, y_train, y_test)
	GridSearchCV: 网格搜索、交叉验证（超参数调优）
		():
            estimator: knn/
            param_grid: 估计器参数（字典设置每个超参数的数组[]值）
            cv: 几折交叉验证
        return:
        	best_score_: 交叉验证最好的结果
        	best_estimator: 最好的模型（超参数组合）
        	best_params: 最好的超参数组合
        	cv_results_: 每个超参数每次交叉验证的结果
		fit(): 训练数据
			x_train: 特征值
			y_train: 目标值
		score(): 准确率
			x_test: 测试集特征值
			y_test: 测试集目标值

朴素贝叶斯：(sklearn.naive_bayes)
sklearn.naive_bayes
	MultinomialNB: 朴素贝叶斯
		():
			alpha: 平滑系数
		fit():
			x_train:  
			y_train:
		predict():
			x_test: 待预测数据
			return: y_predict
		score(): 计算准确率
			x_test: 测试数据集特征值
			y_test: 测试数据集目标值

k-近邻：(sklearn.neighbors)
sklearn.neighbors
	KNeighborsClassifier: K-近邻算法
		():
			n_neighbors: 邻居数
			algorithm: （指定算法）
		fit():
			x: 特征值
			y: 目标值
		predict(): 预测数据
			x: 测试集特征值
			return: 预测的目标值
		score(): 准确率比较（直接再计算一次）
			x: 测试集特征值
			y: 测试集目标值
			
		

预处理：(sklearn.preprocessing)
sklearn.preprocessing
	MinMaxScaler: 归一化
		(feature_range=(mn,mx),)
		fit_transform():
	
	StandardScaler: 标准化
		()
		fit_transform()
		fit(): （计算出数据的平均值和标准差供transform使用）
		transform(): （利用计算好的平均值和标准差进行转换）
		inverse_transform(): 反向转换
		
	Imputer: 缺失值处理
		(missing_values="NaN", strategy="mean", axis=0)
		fit_transform()

决策树：(sklearn.tree)
sklearn.tree
	DicisionTredClassifier: 决策树
		():
            criterion: gini系数
            maxdepth: 树的深度
            random_state: 随机数种子
    	fit():
    		x_train:
    		y_train:
    	score():
    		x_test():
    		y_test():
    export_graphviz(): 导出决策树（DOT格式）
		estimator: dec、
		out_file:
		feature_names: [特征值名]




		

```























